{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD0UzBIX17JX"
      },
      "source": [
        "# Building our own Face Detection model\n",
        "\n",
        "This model comes along my \"FaceID project\". It's the first part of this project which should classify if a photo contains a face or no. \n",
        "\n",
        "It's a binary classification and uses the bases of a ResNet50 model (pretrained on [ImageNet](https://image-net.org/)) and then, fine tuned with faces drawn from the [LFW database](http://vis-www.cs.umass.edu/lfw/) and backgrounds drawn from [House Rooms dataset](https://www.kaggle.com/datasets/robinreni/house-rooms-image-dataset). \n",
        "\n",
        "This model is known as a *face detection* model (opposed to a *face recognition* model which will see in detail in the next part). It classifies whether there is a face (a person) or not in a image. The output of the model is binary (0 or 1) where 0 means there is no face and 1 there is a face."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "V2zJOYnZEkR1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJdox8nPbugJ",
        "outputId": "08d8013d-3cdc-44b8-ceb4-2c3bac2074f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import glob\n",
        "import pickle\n",
        "import tqdm\n",
        "import random as rd\n",
        "import time\n",
        "\n",
        "import torch\n",
        "from torchinfo import summary\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms\n",
        "\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0L1DXFKxpc2S"
      },
      "outputs": [],
      "source": [
        "class FaceDetection_dataset(Dataset):\n",
        "    def __init__(self, imgset_face:list, imgset_noface:list, val_stride:int=0, isValSet_bool:bool=False, \n",
        "                 isAugment_bool:bool=False, isNormalize_bool:bool=False):\n",
        "        \"\"\"\n",
        "        Class that build the dataset to feed the Pytorch Dataloader \n",
        "\n",
        "        -------------------\n",
        "        Class attributs:\n",
        "            imgset_face: list of PIL images\n",
        "                The list of PIL images with face in it\n",
        "            imgset_face: list of PIL images.\n",
        "                The list of PIL images without face in it (typically random\n",
        "                background found in the 'houseroom dataset').\n",
        "            val_stride: int\n",
        "                Use to select training images and validation images without data\n",
        "                leaks.\n",
        "            isValSet_bool: bool\n",
        "                Boolean to construct a validation dataset\n",
        "            isAugment_bool: bool\n",
        "                Boolean to activate the data augmentation preprocessing\n",
        "            isNormalize_bool: bool\n",
        "                Boolean to activate normalization of each image by its own mean\n",
        "                and std values.\n",
        "        \"\"\"\n",
        "        \n",
        "        self.isAugment_bool = isAugment_bool\n",
        "        self.isNormalize_bool = isNormalize_bool\n",
        "        \n",
        "        self.imgset = imgset_face + imgset_noface        \n",
        "        \n",
        "        label_face = np.ones(len(imgset_face)).tolist()\n",
        "        label_noface = np.zeros(len(imgset_noface)).tolist()\n",
        "        self.labelset = label_face + label_noface\n",
        "        \n",
        "        if isValSet_bool:\n",
        "            assert val_stride > 0, 'val_stride should be greater than zero'\n",
        "            self.imgset = self.imgset[::val_stride]\n",
        "            self.labelset = self.labelset[::val_stride]\n",
        "       \n",
        "        elif val_stride > 0:\n",
        "            del self.imgset[::val_stride]\n",
        "            del self.labelset[::val_stride]\n",
        "\n",
        "    def preprocess(self, img)->torch.Tensor:\n",
        "        transform = torchvision.transforms.Compose([\n",
        "            torchvision.transforms.Resize((256, 256)), \n",
        "            torchvision.transforms.ToTensor(),\n",
        "            torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "            ])\n",
        "        img_t = transform(img)\n",
        "        \n",
        "        if self.isAugment_bool:\n",
        "            \"\"\" to do \"\"\"\n",
        "            augment = torchvision.transforms.Compose([\n",
        "                torchvision.transforms.CenterCrop(224),\n",
        "                torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
        "                torchvision.transforms.RandomRotation(degrees=(-10,10))\n",
        "            ])\n",
        "            img_t = augment(img_t)\n",
        "        \n",
        "\n",
        "        if self.isNormalize_bool:\n",
        "            mean, std = img_t.mean(), img_t.std()\n",
        "            img_t = (img_t - mean) / std\n",
        "\n",
        "        return img_t\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgset)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        image = self.imgset[idx]\n",
        "        image = self.preprocess(image)\n",
        "        \n",
        "        label = self.labelset[idx]\n",
        "            \n",
        "        return image, torch.tensor(label).to(torch.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNVduGWRcVPR"
      },
      "source": [
        "# Loading data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_nMN_AjAtib"
      },
      "source": [
        "### Working in the CLOUD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcYczGcZAtib"
      },
      "outputs": [],
      "source": [
        "### https://www.kaggle.com/datasets/robinreni/house-rooms-image-dataset\n",
        "### http://vis-www.cs.umass.edu/lfw/\n",
        "\n",
        "data_path = '/content/drive/MyDrive/Colab Notebooks'\n",
        "\n",
        "#----------------------------- Loading images with a face ---------------------#\n",
        "#imgset_face_path = glob.glob(data_path + '/face_verification/*dataset/*lfw/*/*')\n",
        "#imgset_face_PIL = [Image.open(k).convert('RGB') for k in imgset_face_path]\n",
        "with open(data_path + '/face_verification/dataset/dataset_augmented/lfw_PIL.pkl', 'rb') as lfw_PIL:\n",
        "    imgset_face_PIL = pickle.load(lfw_PIL)\n",
        "\n",
        "\n",
        "#----------------------------- Loading images without face --------------------#\n",
        "#imgset_noface_path = glob.glob(data_path + '/face_verification/*dataset/*houseroom/*/*')\n",
        "#imgset_noface_PIL = [Image.open(k).convert('RGB') for k in imgset_noface_path]\n",
        "with open(data_path + '/face_verification/dataset/houseroom/houseroom_3000_PIL.pkl', 'rb') as houseroom_PIL:\n",
        "    imgset_noface_PIL = pickle.load(houseroom_PIL)### https://www.kaggle.com/datasets/robinreni/house-rooms-image-dataset\n",
        "### http://vis-www.cs.umass.edu/lfw/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-1Wj4cmAtic"
      },
      "source": [
        "### Working in LOCAL MACHINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qj4vKVRGAtid"
      },
      "outputs": [],
      "source": [
        "data_path = '/Users/thierryksstentini/Downloads/dataset'\n",
        "\n",
        "#----------------------------- Loading images with a face ---------------------#\n",
        "with open(data_path + '/dataset_augmented/lfw_PIL.pkl', 'rb') as lfw_PIL:\n",
        "    imgset_face_PIL = pickle.load(lfw_PIL)\n",
        "\n",
        "\n",
        "#----------------------------- Loading images without face --------------------#\n",
        "with open(data_path + '/houseroom_3000_PIL.pkl', 'rb') as houseroom_PIL:\n",
        "    imgset_noface_PIL = pickle.load(houseroom_PIL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_4D2oPbo2k8"
      },
      "outputs": [],
      "source": [
        "#----------------------------- Creating datasets ------------------------------#\n",
        "dataset_train = FaceDetection_dataset(imgset_face_PIL, imgset_noface_PIL, val_stride=10, isValSet_bool=False, isAugment_bool=True)\n",
        "dataset_val = FaceDetection_dataset(imgset_face_PIL, imgset_noface_PIL, val_stride=10, isValSet_bool=True)\n",
        "\n",
        "del imgset_face_PIL, imgset_noface_PIL\n",
        "\n",
        "#----------------------------- Creating loaders -------------------------------#\n",
        "BATCH_SIZE = 64\n",
        "train_dataloader = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(dataset_val, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfVzyPJKcY50"
      },
      "source": [
        "# Creating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlg65G3rAtif"
      },
      "outputs": [],
      "source": [
        "class resNet50_custom(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(resNet50_custom, self).__init__()\n",
        "        self.res50 = torchvision.models.resnet50(pretrained = True, progress = True)\n",
        "        for param in self.res50.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.res50.fc = nn.Linear(2048, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.res50(input)\n",
        "        return torch.sigmoid(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xR-FcCXUAtig"
      },
      "outputs": [],
      "source": [
        "#device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "resNet50_model = resNet50_custom()\n",
        "optimizer = torch.optim.Adam(resNet50_model.parameters(), lr=0.001)\n",
        "num_epochs = 3\n",
        "summary(resNet50_model, (32, 3, 224, 224));"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEX52vLYAtig",
        "outputId": "8f0384da-8de9-4ac3-afc1-0f11f2f8cc2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "------------------------------------\n",
            "Execute notebook on - mps -\n",
            "------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### Choosing device between CPU or GPU\n",
        "#device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "device = torch.device('mps') if torch.has_mps else torch.device('cpu')\n",
        "print(\"\\n------------------------------------\")\n",
        "print(f\"Execute notebook on - {device} -\")\n",
        "print(\"------------------------------------\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5JlRfaDcfOV"
      },
      "source": [
        "# Building the training loop function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMXZknaVuWer"
      },
      "outputs": [],
      "source": [
        "def model_loop(model, epochs, trainloader, validloader, optimizer, loss_fn, device):\n",
        "    model.to(device)\n",
        "    train_loss_list = []\n",
        "    valid_loss_list = []\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch+1} on {device} \\n-------------------------------\")\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        size = len(trainloader.dataset)\n",
        "        \n",
        "        for batch, (data, labels) in enumerate(trainloader):\n",
        "            # Transfer Data to GPU if available\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "\n",
        "            # Clear the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Make prediction & compute the mini-batch training loss\n",
        "            preds = model(data)\n",
        "            loss = loss_fn(preds, labels.unsqueeze(1))\n",
        "\n",
        "            # Compute the gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Update Weights\n",
        "            optimizer.step()\n",
        "\n",
        "            # Aggregate mini-batch training losses\n",
        "            train_loss += loss.item()\n",
        "            train_loss_list.append(train_loss)\n",
        "\n",
        "            \n",
        "            if batch == 0 or batch % 10 == 0:\n",
        "                loss, current = loss.item(), (batch+1) * len(data)\n",
        "                print(f\"mini-batch loss for training : {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "        \n",
        "        # Compute the global training loss as the mean of the mini-batch training losses\n",
        "        train_loss /= len(trainloader)\n",
        "      \n",
        "        model.eval()\n",
        "        valid_loss = 0.0\n",
        "        # Test part : no gradient update\n",
        "        with torch.no_grad():\n",
        "            for batch, (data, labels) in enumerate(validloader):\n",
        "                # Transfer Data to GPU if available\n",
        "                data, labels = data.to(device), labels.to(device)\n",
        "                # Forward Pass & compute the mini-batch validation loss\n",
        "                preds = model(data)\n",
        "                loss = loss_fn(preds,labels.unsqueeze(1))\n",
        "\n",
        "                # Calculate Loss\n",
        "                valid_loss += loss.item()\n",
        "                valid_loss_list.append(valid_loss)\n",
        "\n",
        "                if batch==0 or batch % 10 == 0:\n",
        "                    loss, current = loss.item(), (batch+1) * len(data)\n",
        "                    print(f\"mini-batch loss for validation : {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "        \n",
        "        # Compute the global validation loss as the mean of the mini-batch validation losses\n",
        "        valid_loss /= len(validloader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1} \\n Training Loss: {train_loss:>7f} \\n Validation Loss: {valid_loss:>7f}\" )\n",
        "\n",
        "    return train_loss_list, valid_loss_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFfMqBzKcldY"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tECr8i3nuoGM",
        "outputId": "82cd1475-32f4-459f-a4be-9452c773b61e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 on mps \n",
            "-------------------------------\n",
            "mini-batch loss for training : 0.688751  [   64/ 5536]\n",
            "mini-batch loss for training : 0.238309  [  704/ 5536]\n",
            "mini-batch loss for training : 0.100315  [ 1344/ 5536]\n",
            "mini-batch loss for training : 0.055533  [ 1984/ 5536]\n",
            "mini-batch loss for training : 0.035146  [ 2624/ 5536]\n",
            "mini-batch loss for training : 0.033834  [ 3264/ 5536]\n",
            "mini-batch loss for training : 0.021570  [ 3904/ 5536]\n",
            "mini-batch loss for training : 0.016549  [ 4544/ 5536]\n",
            "mini-batch loss for training : 0.031210  [ 5184/ 5536]\n",
            "mini-batch loss for training : 0.023273  [ 2784/ 5536]\n",
            "mini-batch loss for validation : 0.026839  [   64/ 5536]\n",
            "mini-batch loss for validation : 0.028523  [  400/ 5536]\n",
            "Epoch 1 \n",
            " Training Loss: 0.099469 \n",
            " Validation Loss: 0.024951\n",
            "Epoch 2 on mps \n",
            "-------------------------------\n",
            "mini-batch loss for training : 0.021280  [   64/ 5536]\n",
            "mini-batch loss for training : 0.015328  [  704/ 5536]\n",
            "mini-batch loss for training : 0.014916  [ 1344/ 5536]\n",
            "mini-batch loss for training : 0.013145  [ 1984/ 5536]\n",
            "mini-batch loss for training : 0.008475  [ 2624/ 5536]\n",
            "mini-batch loss for training : 0.012848  [ 3264/ 5536]\n",
            "mini-batch loss for training : 0.020165  [ 3904/ 5536]\n",
            "mini-batch loss for training : 0.009196  [ 4544/ 5536]\n",
            "mini-batch loss for training : 0.008460  [ 5184/ 5536]\n",
            "mini-batch loss for training : 0.003658  [ 2784/ 5536]\n",
            "mini-batch loss for validation : 0.010541  [   64/ 5536]\n",
            "mini-batch loss for validation : 0.019593  [  400/ 5536]\n",
            "Epoch 2 \n",
            " Training Loss: 0.013231 \n",
            " Validation Loss: 0.013235\n",
            "Epoch 3 on mps \n",
            "-------------------------------\n",
            "mini-batch loss for training : 0.009207  [   64/ 5536]\n",
            "mini-batch loss for training : 0.006286  [  704/ 5536]\n",
            "mini-batch loss for training : 0.007273  [ 1344/ 5536]\n",
            "mini-batch loss for training : 0.005074  [ 1984/ 5536]\n",
            "mini-batch loss for training : 0.005993  [ 2624/ 5536]\n",
            "mini-batch loss for training : 0.004538  [ 3264/ 5536]\n",
            "mini-batch loss for training : 0.006087  [ 3904/ 5536]\n",
            "mini-batch loss for training : 0.004408  [ 4544/ 5536]\n",
            "mini-batch loss for training : 0.004056  [ 5184/ 5536]\n",
            "mini-batch loss for training : 0.006146  [ 2784/ 5536]\n",
            "mini-batch loss for validation : 0.005330  [   64/ 5536]\n",
            "mini-batch loss for validation : 0.013318  [  400/ 5536]\n",
            "Epoch 3 \n",
            " Training Loss: 0.008862 \n",
            " Validation Loss: 0.008451\n"
          ]
        }
      ],
      "source": [
        "loss = nn.BCELoss()\n",
        "train_loss, valid_loss = model_loop(model = resNet50_model,\n",
        "                                    epochs = num_epochs,\n",
        "                                    trainloader = train_dataloader,\n",
        "                                    validloader = val_dataloader,\n",
        "                                    optimizer = optimizer,\n",
        "                                    loss_fn = loss,\n",
        "                                    device = device) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5IQX8l2RC-4"
      },
      "outputs": [],
      "source": [
        "losses = {'train_loss' : train_loss, 'valid_loss' : valid_loss}\n",
        "import pickle\n",
        "with open(\"losses_resNet50_FaceDetection_VSC.pickle\", \"wb\") as file:\n",
        "    pickle.dump(losses, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpTlf-xIu8WF"
      },
      "outputs": [],
      "source": [
        "def test_accuracy(model, validloader, device):\n",
        "    correct_results_sum = 0\n",
        "    total = len(validloader.dataset)\n",
        "    model.eval()\n",
        "\n",
        "    # test part : no gradient update\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in validloader :\n",
        "            # Transfer Data to GPU if available\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "            # Prediction on unseen data as 0 or 1\n",
        "            outputs = model(imgs)\n",
        "            predicted = torch.round(outputs).squeeze(1)\n",
        "            print(imgs.shape)\n",
        "            # Aggregate and sum the correct predictions for each mini-batch\n",
        "            correct_results_sum += (predicted == labels).sum().float()\n",
        "\n",
        "    # Mean of the mini-batch correct predictions\n",
        "    acc = correct_results_sum / total\n",
        "    acc = torch.round(acc * 100)\n",
        "    print(\"Validation accuracy : {:.2f}\".format(acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZ4-d1M8vUnZ"
      },
      "outputs": [],
      "source": [
        "test_accuracy(resNet50_model, val_dataloader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0AhVmklAtik"
      },
      "outputs": [],
      "source": [
        "def preprocess_test(img)->torch.Tensor:\n",
        "    transform = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.Resize((256, 256)), \n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    return transform(img)\n",
        "\n",
        "\n",
        "import glob\n",
        "list_img = glob.glob('/Users/thierryksstentini/Downloads/dataset/test2/*')\n",
        "\n",
        "for k in list_img:\n",
        "    img_PIL = Image.open(k).convert('RGB')\n",
        "    frame_preprocess = preprocess_test(img_PIL)\n",
        "    resNet50_model.to('cpu')\n",
        "    resNet50_model.eval()\n",
        "    with torch.no_grad():\n",
        "        pred = resNet50_model(frame_preprocess.unsqueeze(0))\n",
        "    print(\"Prediction en sortie de sigmoid : \", pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4dzqv0CHPUj"
      },
      "outputs": [],
      "source": [
        "torch.save(resNet50_model.state_dict(), 'weights_resNet50_FaceDetection_VSC.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LLavxPbv9cW"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "with open('losses_resNet50_FaceDetection_VSC.pickle', 'rb') as pkl:\n",
        "    losses = pickle.load(pkl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "ldxbH1U1wGgn",
        "outputId": "3ff057c0-59bd-48a5-9462-d9d9ae96645d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x159c55ca0>]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAhSUlEQVR4nO3deXzddZ3v8df3JM2+r03SpEnaUrpAF0ILggyLDoIiyNxREB1k1Drj6EUfc3XwzlVnuXd05npnXIbxyiBXBRUVcGEeCAjIIAKlC91Ll3RJkzR7crKv53v/OCehlKZN25x8v+ec9/PxyKNpcpJ8vvkl73zP9/ddjLUWERHxV8B1ASIicnoKahERzymoRUQ8p6AWEfGcglpExHPJ0fikRUVFtrq6OhqfWkQkLm3ZsqXDWlt8qvdFJairq6vZvHlzND61iEhcMsYcne59GvoQEfGcglpExHMKahERzymoRUQ8p6AWEfGcglpExHMKahERzymo5ZSstexsDPLQK0eZCGkrXBGXorLgRWJXW+8wP3+tiUe3NrK/tR+AiypyWVWZ57YwkQSmoBaGxyb4zZ5WHt3ayAv72wlZWFuVx22XVvLwpmOMh0KuSxRJaArqBDY6HuIHLx/hm88eoHd4nLLcNP786kXcunYBi4qzeGF/Ow9vOoYOARJxS0GdgIJDY3z7+Xp+/GoDwaEx/uCCYj7+9louX1RIUsBMPS5gwq9riFrELQV1gpgIWZ7e3cLjO5p57vU2RsZD3LiyjNvWVXLl4iKMMW/5mMk36VxNEbcU1HHueHCIJ3a28PCrDRxo66coK5X311XygUsrWVGee9qPnQxq9ahF3FJQx6Hg4BhP7DrOL7c1sfFwF9bC8rIc7v3gWt61cv6bhjdOxxB+nEVJLeKSgjqONHQO8u+/O8RPNx9jZDxEbVEmn7nuAm5aVUZtcdZZf77A1NDHLBcqImdFQR0HdjYG+c4L9Tyx8zjJgQDvW1PBHZdVcVFF7inHnmdq8mMV1CJuKahjlLWW3x3o4Dsv1PP7g51kpyaz4apF3HVFNaU5abPyNQJTY9RKahGXFNQxqCU4zCce3Mz2xiAl2al84YYLuX19FTlp82b160zN+pjVzyoiZ0tBHWN2NQX5s4e20DM4xj/+0UXcsqaC1OSkqHwtMzWPWlEt4tKMgtoY81ngY4Q7VzuBu6y1w9EsTN7sQGsf//Z8PY9vb6YwK4UffXw9Fy/Ii+rXnBrdVk6LOHXGoDbGVAD/FVhurR0yxvwUuA34XpRrE6C9b4QHXznK//3PelKTAtyxvorPvvMC8jJSov61A+pRi3hhpkMfyUC6MWYMyACao1eSQPhm4SNbGvnbx/fQPzLODSvn8/e3rKQoK3XOajCanifihTMGtbW2yRjzNaABGAKettY+ffLjjDEbgA0AVVVVs11nQjnY1s8Xf7GLlw91sq6mgH9430UsLjn7edDnSz1qET/MZOgjH7gZqAF6gJ8ZYz5krX3oxMdZa+8D7gOoq6vTb/Y5GB6b4F+fO8h3XqgnfV4S//OWldy+rmrGKwmjRRdTxK2ZDH28AzhsrW0HMMY8BrwNeOi0HyVn5bevt/GlX+3iWNcQt66p4As3LqM4e+6GOU4lMLXgRVEt4tJMgroBuMwYk0F46OM6YHNUq0ogzT1D/N3je3hydwuLS7L48ccv4/JFha7LAjRGLeKLmYxRbzTGPAJsBcaB14gMcci5G5sI8b3fH+FfntlPyFo+d/1SPv72WlKS/TnGUvtRi/hhRrM+rLVfBr4c5VoSxqYjXXzxF7t4vaWPdywr4cs3raCyIMN1WW/xxspEJbWIS1qZOIcOtffzL88c4PHtzVTkpfOdD1/C9Svmuy5rWgHtRy3iBQX1HBgdD/Ht5+u597cHSU4yfOqaxXzymkVkpPj+7dfNRBEf+J4UMW9rQzf3PLqD/a393LSqnC+9Z7nz2Rwzpf2oRfygoI6SgZFxvvb0Pr730hHm56Tx3TvruG5Zqeuyzoo2ZRLxg4I6Cp7f18Zf/3wXzcEhPnzZQj53/VKyZ3kL0rmgHrWIHxTUs2hkfIIv/WI3P9l8jMUlWfzsE5dTV13guqxzNnlmonrUIm4pqGdJQ+cgn/3pNrYc7eaTVy/i7ncsido+0XNFBweI+EFBfZ5CIcuDrxzlq79+neSA4Vu3r+GmVeWuy5oVgYBmfYj4QEF9HkbHQ3z+ke38Ylszf3BBMV+59SLK89JdlzVrJreCUk6LuKWgPkd9w2P8+UNbefFgB5+7fimfvHrReZ347SMtIRfxg4L6HLT1DfORBzaxv7WPr/3xKv7LJQtclxQVWkIu4gcF9Vmqb+/nzgdepWtglPvvrOPqpSWuS4oaoyXkIl5QUJ+FA619fOC+VzDAwxsui/rhsq5NTs/TILWIWwrqGTrSMcAd928kOWD4yScup6Yo03VJUadNmUT8oKCegcbuQe64fyPjIctPNlyWECENbywh1/Q8Ebf82aXeU229w3zo/o30DY/xgz9dx5LSbNclzRn1qEX8oKA+jc7+Ee64fyPtfSN870/XsbIi13VJc2pyjFo5LeKWhj6mMT4R4s8e2kJD1yDfu2sda6vyXZc050zkz7iGPkTcUlBP41vPHWTTkW6+/oHV3hw2O9e0MlHEDxr6OIVHtjTyjWcP8EdrF3DLmgrX5TgT0H7UIl5QUJ9k46FO/urRHbx9SRH/cOtK1+U4pd3zRPygoD5B18Aof/GjrSwsyODf7lgb89uUni/1qEX8oDHqE3znhXo6B0Z58KPrY/JElmhRTou4pR51RGf/CD946SjvXVXOsrIc1+V4IaAFLyJeUFBHfP/lowyPT/Dpa5e4LsUbRlt9iHhBQU34rMMfbTzKtUtLWFyS5bocb2g/ahE/KKiBx7Y20dE/yl1X1LguxStT86g170PEqYQP6n0tffz9f+yhbmE+VyxOzIUt09F+1CJ+SOigDg6OseHBzWSmJnPvHWvj7iit82U0SC3ihYQN6omQ5e6fvEZzzxDfvmMtpTlprkvyUsCoRy3iWsIG9def2c/z+9r58k0rqKsucF2Ot4wxWvAi4lhCBvXWhm6+9dxB3l+3gDvWV7kux2sBoyXkIq4lXFBPhCxf/uVuSnNS+dJNKzQufQYG9ahFXEu4oH54UwM7m4L89xuXkZWqFfRnYgzqUos4llBB3T0wyv9+ah/rawp476py1+XEBGO0KZOIawkV1P/01D76hsf5u5tXashjhgLGaHaeiGMJE9Q7Gnt4eFMDH3lbNUvnJ84BtecrYIym54k4NqOgNsbkGWMeMca8bozZa4y5PNqFzbavPPE6hZkpfOYd2nTpbISHqJXUIi7NtEf9DeBJa+2FwCpgb/RKmn0v1Xfw8qFOPnn1Yu0zfZaM0cJEEdfOOO3BGJMLXAV8BMBaOwqMRres2fXNZw9QmpPKBzVn+qwZY7QftYhjM+lR1wDtwP8zxrxmjLnfGJN58oOMMRuMMZuNMZvb29tnvdBztaspyCuHuvjYlbWkzUvso7XOhZaQi7g3k6BOBtYC37bWrgEGgHtOfpC19j5rbZ21tq64uHiWyzx3333xMJkpSXxgXaXrUmKSMUZj1CKOzSSoG4FGa+3GyP8fIRzc3msJDvP49mY+cGkVORqbPifqUYu4d8agtta2AMeMMUsjb7oO2BPVqmbJ918+Qsha7rqi2nUpMUzzqEVcm+ka6k8DPzTGpACHgLuiV9LsGBgZ54evHOVdK+dTWZDhupyYFTA63FbEtRkFtbV2G1AX3VJm16NbG+kdHuejV+p4rfOh6Xki7sXlysSJkOWBFw+zujKPtVX5rsuJaQHtRy3iXFwG9Qv72znSOchHr6zRnh7nSZvnibgXl0H9083HKMhM4foV812XEvN0wouIe3EX1F0Dozyzt5VbVleQkhx3zZtz2o9axL24S7JfbmtibMLyx3ULXJcSFzRGLeJe3AX1zzY3clFFLsvKclyXEheMzkwUcS6ugnpXU5A9x3vVm55F2o9axL24CurHtjaRkhTQMVuzyKAFLyKuxU1QW2t5ancLV11QRF5Giuty4oYWvIi4FzdBvaupl6aeIf5QU/JmlXbPE3EvboL6qd0tJAUM71hW6rqUuBIwEAq5rkIkscVNUD+5u4V11QUUZGrYYzYZND1PxLW4COr69n4OtvVz/Qr1pmebpueJuBcXQf3U7hYAjU9Hgc5MFHEvToK6lVULcinPS3ddStwJaNaHiHMxH9TBwTF2NPZwzYUlrkuJS8agMWoRx2I+qF890oW1cHltoetS4lLAGI1RizgW80G98VAnKckBVlXmuS4lLhktIRdxLuaD+pXDnaytyiNtXpLrUuKSlpCLuBfTQR0cGmNPcy/razTsES26mSjiXkwH9eYjXYQsXKbx6ajREnIR92I6qDce7iIlKcCaqjzXpcQtLSEXcS+2g/pQJ6s1Ph1VBvWoRVyL2aDuGx5jZ1OQy2oKXJcS18LzqF1XIZLYYjaoX2voIWRhnW4kRpUOtxVxL2aDekdjDwAXV+a6LSTO6XBbEfdiNqi3NwapLc4kJ22e61LimnbPE3EvZoN6Z2OQiyvUm4429ahF3IvJoG7rHaald5iLF+S5LiUhKKdF3IrJoN7RGATg4gXqUUdbQPtRizgXo0HdQ1LAsKJcQR1tGqMWcS82g7opyJKSLNJTtNAl2jRGLeJezAW1tZYdjUENe8yR8O55rqsQSWwxF9RNPUN0DYzqRuIc0X7UIu7FXFDrRuLcMkb7UYu4FpNBnZIUYOn8bNelJATtRy3iXswF9f7WPmqLM0lN1o3EuaDd80Tcm3FQG2OSjDGvGWP+I5oFncmBtj6WlKo3PVcCAe2eJ+La2fSo7wb2RquQmRganaCxe4glJVkuy0goBi14EXFtRkFtjFkAvBu4P7rlnF59ez/WwmIF9ZwxGqMWcW6mPeqvA58Hpj2UyRizwRiz2Rizub29fTZqe4uDbf0A6lHPIaMFLyLOnTGojTHvAdqstVtO9zhr7X3W2jprbV1xcfGsFXiiA219JAcMCwszo/L55a0CWkIu4txMetRXAO81xhwBHgauNcY8FNWqpnGwrZ+FhRmkJMfcZJWYZUA9ahHHzph41tovWGsXWGurgduA56y1H4p6ZadwsK2fRcUa9phL4d3zXFchkthipms6PhGioWuQWgX1nDIKahHnks/mwdba54Hno1LJGTR2DzE2Yakt1vj0XNISchH3YqZHfagjPONjkYJ6TgWMFryIuBY7Qd0+AEBtkYY+5pKWkIu4FzNBXd8+QH7GPPIzU1yXklC0hFzEvZgJ6kPt/bqR6IRuJoq4FjtB3TFATZHGp+daQDcTRZyLiaDuGx6jvW9EMz4c0OG2Iu7FRFDrRqI7OtxWxL3YCGpNzXNGh9uKuBcTQX24fYCAgarCDNelJBztnifiXkwEdX3HAJUFGTp+ywFj0CC1iGMxEdSH2jXjwxWNUYu4531Qh0KWwx39upHoiDrUIu55H9QtvcMMj4Wo0Y1EJwIB9ahFXPM+qBu6BgFYWKAbiS5o1oeIe94H9bFIUFcpqJ3QftQi7vkf1N1DGAPleemuS0lI4ZWJSmoRl/wP6q5BynPTdU6iI9qPWsQ979PvWNcgC/LVm3bFYLQpk4hj3gd1Q9egxqcdUo9axD2vg3p4bIK2vhEqFdTuGANoq1MRl7wO6sbuIUAzPlwKhHNaMz9EHPI6qI91h6fmaYzaHUOkR+24DpFE5nVQH+8ZBjQ1z6XJHrVWJ4q443VQtwSHCBgoyU51XUrCMgpqEee8Durm4DAl2WkkJ3ldZlwzUzcTHRciksC8TsCW4DBleWmuy0hoRjcTRZzzOqibg0OU5SqoXQpM9qh1O1HEGW+D2lob7lHn6kaiS2/cTHRbh0gi8zaoe4fGGRydUI/asanpeRr7EHHG26BuDoYXu6hH7ZZRj1rEOW+DuiUYnkM9Xz1qp8zU3US3dYgkMm+DerJHXa5ZH05pwYuIe94GdUtwmKSAoSRbQe1SJKfVoRZxyNugbu4ZpiQ7laTJLp04EYh8/9WjFnHH26A+rjnUXpjqUSunRZzxNqg1h9oPRvtRizh3xqA2xlQaY35rjNljjNltjLk72kVZa7Uq0ROa9CHiXvIMHjMO/KW1dqsxJhvYYoz5jbV2T7SKCg6NMTwW0tQ8D0wuIdcYtYg7Z+xRW2uPW2u3Rl7vA/YCFdEsqln7UHtDY9Qi7p3VGLUxphpYA2w8xfs2GGM2G2M2t7e3n1dRLb2TqxLVo3ZNPWoR92Yc1MaYLOBR4DPW2t6T32+tvc9aW2etrSsuLj6voiZ71LqZ6AFtcyri3IyC2hgzj3BI/9Ba+1h0SwpPzUsKGIp1sotzAR0cIOLcTGZ9GOC7wF5r7T9HvyRoCY5osYsn3liZqKQWcWUmPeorgA8D1xpjtkVeboxmUZ0DI+pNeyIQ+QnR7nki7pxxep619kXe6FjNiY7+EYqzFNQ+0H7UIu55uTKxs3+UQgW1F7QftYh73gW1tZbO/lGKFNRemNqPWmPUIs54F9S9w+OMToQoykpxXYqgMxNFfOBdUHf2jwBQqKD2whtj1I4LEUlg/gX1wCgAhZka+vCBTngRcc+/oI70qDVG7QejlYkiznkX1O394R61xqj9YLTXh4hz3gX1ZI86P1NB7QPtnifinodBPUpexjzmJXlXWkKa2utD0/NEnPEuDTsHRihUb9obWkIu4p53Qd3Rp8UuPtESchH3/AvqgREFtUe0hFzEPe+COrzPh4Y+fKEl5CLuzeRw2zkzOh4iODSmxS4e0RJyd4bHJtjf2seh9gEOtfdT3zFAc88Qbb0jZKcl8/inr9RN9wThVVB3RVYlFmWrR+0LLSGfW219w/x44zGe3tPCvpY+xiN/IQMGKgsyWJCfTmFWCjsag3QPjlKSrXNFE4FXQd0xuc+HetTe0BLy6GnrG2ZXU5Cdjb3sag6yqynI8WD4vND1NQVsuKqWiypyWVySRVVhBqnJSQA8uqWRv/zZdgZHJiDbZQtkrngV1JP7fGhVoke0hHxWjIxPsKspyOYj3Ww60s2Oxh7a+sIdE2OgpiiTdTUFrCzP5eqlxSwpnT6BM1PDgT04OjEntYt7fgX11M556lH74o3DbZXUMxUKWQ53DrCrKdxL3nash+2NQUbHQ0A4lK9YXMTKilwuqshleXkOWakz/1VMTwk/dmhsPCr1i3+8CuqOqQ2Z1KP2heZ8nJ61lpbeYQ63D1DfMcCe5iBP7mqhe3AMgJTkAMvLcrjz8oVcsrCASxbmn/d5oJkp4R71wIh61InCq6Du7B8lJTlwVr0Lia5AQJsynawlOMwL+9t5dGsjO5uCbxqCyExJ4tplpbw90mNeUpo16zMz0lM09JFovErEjv5RijJTTpi7K65pUyboHR7j9wc6eKm+k9/Xd3CofQAID2G8v66SRcWZ1BZnUVucyfyctKj//GZGhj4GRzX0kSi8CurOgRGNT3smUbc5be4ZYk9zL7/c3sxTu1sYHQ+RkZLEupoCbr+0irctLmR5WY6TTkWGetTeCQ6N0dwzRO/QGOtrC2f983sV1B39IxQrqL0ydXCA2zKizlrL7uZentrdwpO7WjjQ1g9Abvo8bru0kptWlbO6Ms+LBSYZqepRzyVrLRsPd3GgtY+O/lE6+kciL5HX+0YYiPzRLMxMYcsX3znrNXgV1J39o1w4P8d1GXKCeJ31Ya2loWuQbcd62Hq0m2f2ttHUM0TAwLqaAm5bt5wV5Tmsqcqbmr/si/R56lFH2+Rw19GuQZ7c1cK2Yz1AuOOSn5FCUVYKRVmpXLwgj6KsFMpy06jIy6AiPz0q9XgT1NZa7fPhoXgZo27rG2ZPcy87G4O8dqyHbcd6plbCps9L4vJFhdx93RKuW1bi/fBbUsCQNi+goJ4lo+MhmnqGONzRz6H2AZp6hnh8+/GpWWgXlGbxv963kncuK6UgM4VkB8+qvAlqgBc+fw3JSbqR6JPA1Bi140LOwthEiJfrO3nxYAd7j/ey93gvHZEj3gCWlGRx3YUlrK7KY01lPheUZjn55TsfGSnJGvo4S8NjEzy/r439rf00dA1yrGuQxu4hjgeH3vTznZGSxEUVufzrB9dwQWk2BR7sj+9NUBtjmJ+rfQt888bhtv4m9dhEiD3NvWw52s3Whm5+d6CD4NAYKckBLijN4pqlJSwvz2FZWQ7Ly3PISZvnuuTzlpGSFF5CLjOyqynInzzw6tSzqNKcVKoKMlhfU8CCggwq89OpLc6ktijLy2MAvQlq8ZOP+1EHB8fY2tDN5qNdbD7SzfbGHobHwqv+ynPTuG5ZCTesLOPtS4pIm+fX+PJsyUhJ0tDHNMYmQrT3jXA8OMzx4BDNPUM88OIR0pID/PBj67lkYX7M/VwoqOW0jOO1idZajnYOsvloN1siwTw5IyMpYFhRnsPt66q4ZGE+dQsLEuZZWUZKMgMJOvQxMDLO8eAwrb3DtASHaekd5kjHAPta+zgeHKajf+Qt91RKc1K5/851LC+PzckKCmo5LRdnJh4PDk2NMb90sJOW3vCOctlpyVyyMJ+bV5ezdmE+qyvzyEhJzB/hjJQkhuK0Rz0wMs6rR7pojYTu5DS4luAwRzoH3nS/YVJhZkp4eGt+DqW5aczPSaMsN42yvDTKctPJSUuO6YV0iflTLjM2F/tRN3YP8vy+dl6q7+C1hp6prT7zM+bxtsVFXF5byKXVBSwpyZpa0p7oMlKS6R4ccl3GeekZHOW1Yz0c7RjgWPcQx7oGOdY9xKH2fkYiG1gBZKcmU5SdSkl2KtddWMrCogzKc9MpzUljfm4apTmpcf8HO75bJ+ctWvtR72/t47GtTTyzt5WDkaGMirx06qoLWF2Zx2W1BSybn6Ngnka4Rx0bQx/DYxP85/526tv7aY0MVRxqH5gawoLwFMkF+elUFmRwxaJCrrmwhIWFGRRlpcbceHI0KKjltGZzZeJEyPLs3lbu/e1BtjcGSQ4YLl9UyG2XVnL10hIWFWfG9NPTuZSZ6ufNxImQ5WBbPzubguxs7GFnU5A9x3unbvZmpyVTlptGZUEGt6ypYG1VPktKsyjUHj+npaCW0zLnsTIxFLJsbejm5fpONh3tZuvRbvpHxqkpyuSL71nOzavLdeL8OUqfl+w8qK21dA+OcbhjgMMdA+xv7eMXrzVNHYiQkZLEyvJcPrhuIdctK2FNVeLeUzhf+q7JaZ3tysTB0XFePNDBc6+38ezrbbRHfmmXlmZzy5pyLq8t4voVpTG3wMQ34R71ONbaOemJtvYOs+lIF/VtAxzu6Odw5yCH2/vpHX5j+CUpYHj7kiL+6uJyVlXmUlOURZKGrmaFglpOKzCD3fMauwfDwby3jZcPdTI6HiI7NZmrLijm+pXzuWpJEXkZ/i0iiGXpKUmELIyMh2ZlDNdaS0f/KEc7BzjSOfimf492DhIcCh+EYAyU56ZTU5TJe1eXU1OURU1RBjVFWSzIT/di06p4pKCW0zrVghdrw0Maz+5t47nX23i9pQ+A6sIMPnxZ+GnupdUF+qWNojf2pJ4466AODo7x613HOdjWT2P3EEe7woF84lBKwEBFfjrVhZnctKqMmqIs1lUXsKQ0Szf3HJhRUBtj3gV8A0gC7rfWfjWqVYk3Tt4975VDnXzlib1sbwySFDCsqy7gf7x7GddeWEJtcZbLUhNK+tRxXOOn3Iuif2Schs5BGrrCPeKGrvDL0c5BmnqGmAhZ0uYFWJAfXj69vqaA6sIMFhZlUl2YSUVeOinJ+kPrizMGtTEmCbgXeCfQCGwyxvzKWrsn2sWJe5M96mf2tvLIlkY2Hu6iLDeNr956ETdcVEZueuzvmxGLJnvUA6PjtASHaegaZEdjDy/Xd7K9secti0LyMuaxsCCDixfkcvPqcq5fMZ8V5W4OPpCzN5Me9TrgoLX2EIAx5mHgZkBBnQAm92J+ancri0uyuOeGC/nI26r19NexyVNe3v3NF5k4YVyqtiiTa5aGn91UFWSwsDCDyoIM/UGNcTMJ6grg2An/bwTWn/wgY8wGYANAVVXVrBQn7hVnp/LQR9dTlpfGIg1teGPtwnxuX1dJTto8KgvCYby0NDth9jpJNLN2M9Faex9wH0BdXZ1He63J+bpySZHrEuQkuenz+MqtF7suQ+bITO4WNAGVJ/x/QeRtIiIyB2YS1JuAJcaYGmNMCnAb8KvoliUiIpPOOPRhrR03xnwKeIrw9LwHrLW7o16ZiIgAMxyjttY+ATwR5VpEROQUNKNdRMRzCmoREc8pqEVEPKegFhHxnDmXDeHP+EmNaQeOnuOHFwEds1iOrxKlnZA4bU2UdkLitHUu27nQWlt8qndEJajPhzFms7W2znUd0ZYo7YTEaWuitBMSp62+tFNDHyIinlNQi4h4zsegvs91AXMkUdoJidPWRGknJE5bvWind2PUIiLyZj72qEVE5AQKahERz3kT1MaYdxlj9hljDhpj7nFdz2wzxhwxxuw0xmwzxmyOvK3AGPMbY8yByL/5rus8F8aYB4wxbcaYXSe87ZRtM2HfjFznHcaYte4qPzvTtPNvjDFNkeu6zRhz4wnv+0KknfuMMde7qfrsGWMqjTG/NcbsMcbsNsbcHXl7PF7T6drq13W11jp/Ibx9aj1QC6QA24Hlruua5TYeAYpOets/AfdEXr8H+EfXdZ5j264C1gK7ztQ24Ebg14ABLgM2uq7/PNv5N8B/O8Vjl0d+jlOBmsjPd5LrNsywnWXA2sjr2cD+SHvi8ZpO11avrqsvPeqpA3SttaPA5AG68e5m4PuR178P3OKulHNnrX0B6DrpzdO17WbgBzbsFSDPGFM2J4Wep2naOZ2bgYettSPW2sPAQcI/596z1h631m6NvN4H7CV8dmo8XtPp2jodJ9fVl6A+1QG6p/tmxSILPG2M2RI5CBig1Fp7PPJ6C1DqprSomK5t8XitPxV5yv/ACcNXcdFOY0w1sAbYSJxf05PaCh5dV1+COhFcaa1dC9wA/IUx5qoT32nDz6vicq5kPLcN+DawCFgNHAf+j9NqZpExJgt4FPiMtbb3xPfF2zU9RVu9uq6+BHXcH6BrrW2K/NsG/Jzw06XWyaeIkX/b3FU466ZrW1xda2ttq7V2wlobAv6dN54Gx3Q7jTHzCAfXD621j0XeHJfX9FRt9e26+hLUcX2ArjEm0xiTPfk68IfALsJtvDPysDuBX7qpMCqma9uvgD+JzBS4DAie8HQ65pw0Fvs+wtcVwu28zRiTaoypAZYAr851fefCGGOA7wJ7rbX/fMK74u6aTtdW766r67uuJ9xNvZHwHdd64K9d1zPLbaslfKd4O7B7sn1AIfAscAB4BihwXes5tu/HhJ8ejhEes/vodG0jPDPg3sh13gnUua7/PNv5YKQdOwj/Eped8Pi/jrRzH3CD6/rPop1XEh7W2AFsi7zcGKfXdLq2enVdtYRcRMRzvgx9iIjINBTUIiKeU1CLiHhOQS0i4jkFtYiI5xTUIiKeU1CLiHju/wNnAs1ceWWZVwAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_loss = losses['train_loss']\n",
        "valid_loss = losses['valid_loss']\n",
        "\n",
        "train_loss_a = np.array(train_loss)\n",
        "#valid_loss_a = np.array(valid_loss)\n",
        "\n",
        "plt.plot(train_loss_a)\n",
        "#plt.plot(valid_loss_a)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Face_detection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "bfc2e043f4a41bbce066f7921047bdcfe8769546c7198e4df6482fd90de274f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}