{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD0UzBIX17JX"
      },
      "source": [
        "# Building our own Face Detection model\n",
        "\n",
        "This model comes along my \"FaceID project\". It's the first part of this project which should classify if a photo contains a face or no. \n",
        "\n",
        "It's a binary classification and uses the bases of a ResNet50 model (pretrained on [ImageNet](https://image-net.org/)) and then, fine tuned with faces drawn from the [LFW database](http://vis-www.cs.umass.edu/lfw/) and backgrounds drawn from [House Rooms dataset](https://www.kaggle.com/datasets/robinreni/house-rooms-image-dataset). \n",
        "\n",
        "This model is known as a *face detection* model (opposed to a *face recognition* model which will see in detail in the next part). It classifies whether there is a face (a person) or not in a image. The output of the model is binary (0 or 1) where 0 means there is no face and 1 there is a face."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2zJOYnZEkR1"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "ZJdox8nPbugJ",
        "outputId": "9f1e0367-95e4-45c9-e4b3-2c42cb6bf61d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import glob\n",
        "import pickle\n",
        "import tqdm\n",
        "import random as rd\n",
        "import time\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision\n",
        "\n",
        "from torchinfo import summary\n",
        "\n",
        "\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS0pE3UuOvJx"
      },
      "source": [
        "# Creating dataset class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9S9gug5P9FP"
      },
      "source": [
        "The **FaceDetection_dataset** class is used to create a Pytorch instance of the dataset. It should handle several things :\n",
        "*   Returns training and validation dataset\n",
        "*   Returns Pytorch Tensors\n",
        "*   Augments the dataset\n",
        "*   Normalizes the dataset\n",
        "\n",
        "I chose to augment the dataset with random horizontal flips and random rotations to simulate subject's positionning imperfections.\n",
        "\n",
        "Also, as suggests the [ResNet paper](https://arxiv.org/abs/1512.03385), and as recalls the [Pytorch websibe](https://pytorch.org/hub/pytorch_vision_resnet/) : when using pretrained ResNet, image pixel values should be between `[0,1]` and one should normalize the images using `mean = [0.485, 0.456, 0.406]` and `std = [0.229, 0.224, 0.225]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "x6RVuynuJvZE"
      },
      "outputs": [],
      "source": [
        "class FaceDetection_dataset(Dataset):\n",
        "    def __init__(self, imgset_face:list, imgset_noface:list, split:str, isValSet_bool:bool=False, \n",
        "                 isAugment_bool:bool=False, isNormalize_bool:bool=True):\n",
        "        \"\"\"\n",
        "        Class that build the dataset to feed the Pytorch Dataloader \n",
        "\n",
        "        -------------------\n",
        "        Class attributs:\n",
        "            imgset_face: list of PIL images\n",
        "                The list of PIL images with face in it\n",
        "            imgset_face: list of PIL images.\n",
        "                The list of PIL images without face in it (typically random\n",
        "                background found in the 'houseroom dataset').\n",
        "            split: str\n",
        "                Used to select training set or validation set\n",
        "            isValSet_bool: bool\n",
        "                Boolean to construct a validation dataset\n",
        "            isAugment_bool: bool\n",
        "                Boolean to activate the data augmentation preprocessing\n",
        "            isNormalize_bool: bool\n",
        "                Boolean to activate normalization of each channel by mean and \n",
        "                std ResNet paper values.\n",
        "        \"\"\"\n",
        "        \n",
        "        self.isAugment_bool = isAugment_bool\n",
        "        self.isNormalize_bool = isNormalize_bool\n",
        "\n",
        "        split_pct = float(split.strip('%'))/100\n",
        "        len_imageset_face = round(len(imgset_face) * split_pct)\n",
        "        len_imageset_noface = round(len(imgset_noface) * split_pct)\n",
        "\n",
        "        \n",
        "\n",
        "        if isValSet_bool == False:\n",
        "            imgset_face = imgset_face[:len_imageset_face] \n",
        "            imgset_noface = imgset_noface[:len_imageset_noface]\n",
        "        else :\n",
        "            imgset_face = imgset_face[-len_imageset_face:]\n",
        "            imgset_noface = imgset_noface[-len_imageset_noface:]\n",
        "        \n",
        "        self.imgset = imgset_face + imgset_noface\n",
        "\n",
        "        label_face = np.ones(len(imgset_face)).tolist()\n",
        "        label_noface = np.zeros(len(imgset_noface)).tolist()\n",
        "        self.labelset = label_face + label_noface\n",
        "        \n",
        "\n",
        "    def preprocess(self, img)->torch.Tensor:\n",
        "        transform = torchvision.transforms.Compose([\n",
        "            torchvision.transforms.Resize((256, 256)), \n",
        "            torchvision.transforms.ToTensor()])\n",
        "        img_t = transform(img)\n",
        "        \n",
        "        if self.isAugment_bool:\n",
        "            augment = torchvision.transforms.Compose([\n",
        "                torchvision.transforms.CenterCrop((224,224)),\n",
        "                torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
        "                torchvision.transforms.RandomRotation(degrees=(-10,10))])\n",
        "            img_t = augment(img_t)\n",
        "\n",
        "        if self.isNormalize_bool:\n",
        "            normalize = torchvision.transforms.Normalize(\n",
        "                mean=[0.485, 0.456, 0.406], \n",
        "                std=[0.229, 0.224, 0.225])\n",
        "            img_t = normalize(img_t)\n",
        "        return img_t\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgset)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        image = self.imgset[idx]\n",
        "        image_t = self.preprocess(image)\n",
        "        \n",
        "        label = self.labelset[idx]\n",
        "            \n",
        "        return image_t, torch.tensor(label).to(torch.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNVduGWRcVPR"
      },
      "source": [
        "# Loading data\n",
        "To improve data streamflow, datasets are already converted to `PIL.Image` and stored in a `.pkl` file such as : \n",
        "* `lfw_PIL.pkl` contains 3000 random images drawn from LFW database and converted into `PIL.Image`\n",
        "* `houseroom_3000_PIL.pkl` contains 3000 random images drawn from House Rooms Kaggle dataset and converted into `PIL.Image`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_nMN_AjAtib"
      },
      "source": [
        "### Working on the CLOUD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcYczGcZAtib"
      },
      "outputs": [],
      "source": [
        "### https://www.kaggle.com/datasets/robinreni/house-rooms-image-dataset\n",
        "### http://vis-www.cs.umass.edu/lfw/\n",
        "\n",
        "data_path = '/content/drive/MyDrive/Colab Notebooks'\n",
        "\n",
        "#----------------------------- Loading images with a face ---------------------#\n",
        "#imgset_face_path = glob.glob(data_path + '/face_verification/*dataset/*lfw/*/*')\n",
        "#imgset_face_PIL = [Image.open(k).convert('RGB') for k in imgset_face_path]\n",
        "with open(data_path + '/face_verification/dataset/dataset_augmented/lfw_PIL.pkl', 'rb') as lfw_PIL:\n",
        "    imgset_face_PIL = pickle.load(lfw_PIL)\n",
        "\n",
        "\n",
        "#----------------------------- Loading images without face --------------------#\n",
        "#imgset_noface_path = glob.glob(data_path + '/face_verification/*dataset/*houseroom/*/*')\n",
        "#imgset_noface_PIL = [Image.open(k).convert('RGB') for k in imgset_noface_path]\n",
        "with open(data_path + '/face_verification/dataset/houseroom/houseroom_3000_PIL.pkl', 'rb') as houseroom_PIL:\n",
        "    imgset_noface_PIL = pickle.load(houseroom_PIL)### https://www.kaggle.com/datasets/robinreni/house-rooms-image-dataset\n",
        "### http://vis-www.cs.umass.edu/lfw/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-1Wj4cmAtic"
      },
      "source": [
        "### Working on LOCAL MACHINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "qj4vKVRGAtid"
      },
      "outputs": [],
      "source": [
        "data_path = '/Users/thierryksstentini/Downloads/dataset'\n",
        "\n",
        "#----------------------------- Loading images with a face ---------------------#\n",
        "with open(data_path + '/dataset_augmented/lfw_PIL.pkl', 'rb') as lfw_PIL:\n",
        "    imgset_face_PIL = pickle.load(lfw_PIL)\n",
        "\n",
        "\n",
        "#----------------------------- Loading images without face --------------------#\n",
        "with open(data_path + '/houseroom_3000_PIL.pkl', 'rb') as houseroom_PIL:\n",
        "    imgset_noface_PIL = pickle.load(houseroom_PIL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdkVwWR8E7Uf"
      },
      "source": [
        "# Creating datasets and dataloaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "J_4D2oPbo2k8"
      },
      "outputs": [],
      "source": [
        "#----------------------------- Creating datasets ------------------------------#\n",
        "dataset_train = FaceDetection_dataset(imgset_face_PIL, imgset_noface_PIL, split='80%', isValSet_bool=False, isAugment_bool=True)\n",
        "dataset_val = FaceDetection_dataset(imgset_face_PIL, imgset_noface_PIL, split='20%', isValSet_bool=True)\n",
        "\n",
        "del imgset_face_PIL, imgset_noface_PIL\n",
        "\n",
        "#----------------------------- Creating loaders -------------------------------#\n",
        "BATCH_SIZE = 64\n",
        "train_dataloader = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(dataset_val, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfVzyPJKcY50"
      },
      "source": [
        "# Creating the model\n",
        "Here I use a pretrained ResNet50 model from torchvision models. All the layers are frozen (`param.requires_grad = False`) and the last layer is turned into a linear layer (`torch.nn.Linear`) with a unique output unit (since it is a binary classification, only one output is need). \n",
        "\n",
        "Finally, we applied a sigmoid function to the forward output to turn the output into probability. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "qlg65G3rAtif"
      },
      "outputs": [],
      "source": [
        "class resNet50_custom(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(resNet50_custom, self).__init__()\n",
        "        self.res50 = torchvision.models.resnet50(pretrained = True, progress = True)\n",
        "        for param in self.res50.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.res50.fc = torch.nn.Linear(2048, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.res50(input)\n",
        "        return torch.sigmoid(output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XONZeSHSGNrG"
      },
      "source": [
        "# Model compilation\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "xR-FcCXUAtig"
      },
      "outputs": [],
      "source": [
        "resNet50_model = resNet50_custom()\n",
        "BCEloss = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(resNet50_model.parameters(), lr=0.001)\n",
        "num_epochs = 2\n",
        "#summary(resNet50_model, (32, 3, 224, 224));"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "dEX52vLYAtig",
        "outputId": "8f0384da-8de9-4ac3-afc1-0f11f2f8cc2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "------------------------------------\n",
            "Execute notebook on - mps -\n",
            "------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### Choosing device between CPU or GPU\n",
        "\n",
        "#device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "device = torch.device('mps') if torch.has_mps else torch.device('cpu')\n",
        "\n",
        "print(\"\\n------------------------------------\")\n",
        "print(f\"Execute notebook on - {device} -\")\n",
        "print(\"------------------------------------\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5JlRfaDcfOV"
      },
      "source": [
        "# Building the training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFfMqBzKcldY"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "xKqiX7EdHQje"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 on mps \n",
            "-------------------------------\n",
            "mini-batch loss for training : 0.664027  [   64/ 4922]\n",
            "mini-batch loss for training : 0.284062  [  704/ 4922]\n",
            "mini-batch loss for training : 0.121838  [ 1344/ 4922]\n",
            "mini-batch loss for training : 0.077326  [ 1984/ 4922]\n",
            "mini-batch loss for training : 0.069712  [ 2624/ 4922]\n",
            "mini-batch loss for training : 0.032165  [ 3264/ 4922]\n",
            "mini-batch loss for training : 0.038749  [ 3904/ 4922]\n",
            "mini-batch loss for training : 0.019940  [ 4544/ 4922]\n",
            "mini-batch loss for training : 0.016665  [ 4922/ 4922]\n",
            "\n",
            "\n",
            "mini-batch for validation : Loss 0.034736    Acc 100.0%    [   64/ 1230]\n",
            "mini-batch for validation : Loss 0.037042    Acc 100.0%    [  192/ 1230]\n",
            "mini-batch for validation : Loss 0.037074    Acc 100.0%    [  320/ 1230]\n",
            "mini-batch for validation : Loss 0.038113    Acc 100.0%    [  448/ 1230]\n",
            "mini-batch for validation : Loss 0.041563    Acc 100.0%    [  576/ 1230]\n",
            "mini-batch for validation : Loss 0.057581    Acc 100.0%    [  704/ 1230]\n",
            "mini-batch for validation : Loss 0.066127    Acc 100.0%    [  832/ 1230]\n",
            "mini-batch for validation : Loss 0.033848    Acc 100.0%    [  960/ 1230]\n",
            "mini-batch for validation : Loss 0.020973    Acc 100.0%    [ 1088/ 1230]\n",
            "mini-batch for validation : Loss 0.031060    Acc 100.0%    [ 1216/ 1230]\n",
            "mini-batch for validation : Loss 0.033929    Acc 100.0%    [  280/ 1230]\n",
            "\n",
            "\n",
            "Epoch 1 \n",
            " Training Loss: 0.129418 \n",
            " Validation Loss: 0.036883 \n",
            " Accuracy: 100.0%\n",
            "\n",
            "\n",
            "Epoch 2 on mps \n",
            "-------------------------------\n",
            "mini-batch loss for training : 0.020600  [   64/ 4922]\n",
            "mini-batch loss for training : 0.022896  [  704/ 4922]\n",
            "mini-batch loss for training : 0.021507  [ 1344/ 4922]\n",
            "mini-batch loss for training : 0.028752  [ 1984/ 4922]\n",
            "mini-batch loss for training : 0.015626  [ 2624/ 4922]\n",
            "mini-batch loss for training : 0.012309  [ 3264/ 4922]\n",
            "mini-batch loss for training : 0.010927  [ 3904/ 4922]\n",
            "mini-batch loss for training : 0.033253  [ 4544/ 4922]\n",
            "mini-batch loss for training : 0.025811  [ 4922/ 4922]\n",
            "\n",
            "\n",
            "mini-batch for validation : Loss 0.013242    Acc 100.0%    [   64/ 1230]\n",
            "mini-batch for validation : Loss 0.013907    Acc 100.0%    [  192/ 1230]\n",
            "mini-batch for validation : Loss 0.013584    Acc 100.0%    [  320/ 1230]\n",
            "mini-batch for validation : Loss 0.013995    Acc 100.0%    [  448/ 1230]\n",
            "mini-batch for validation : Loss 0.016354    Acc 100.0%    [  576/ 1230]\n",
            "mini-batch for validation : Loss 0.045258    Acc 100.0%    [  704/ 1230]\n",
            "mini-batch for validation : Loss 0.058251    Acc 100.0%    [  832/ 1230]\n",
            "mini-batch for validation : Loss 0.024754    Acc 100.0%    [  960/ 1230]\n",
            "mini-batch for validation : Loss 0.013101    Acc 100.0%    [ 1088/ 1230]\n",
            "mini-batch for validation : Loss 0.021632    Acc 100.0%    [ 1216/ 1230]\n",
            "mini-batch for validation : Loss 0.021768    Acc 100.0%    [  280/ 1230]\n",
            "\n",
            "\n",
            "Epoch 2 \n",
            " Training Loss: 0.021364 \n",
            " Validation Loss: 0.020712 \n",
            " Accuracy: 100.0%\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "resNet50_model.to(device)\n",
        "train_loss_list = []\n",
        "valid_loss_list = []\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1} on {device} \\n-------------------------------\")\n",
        "    resNet50_model.train()\n",
        "    train_loss = 0.0\n",
        "    size_trainset = len(train_dataloader.dataset)\n",
        "    size_valset = len(val_dataloader.dataset)\n",
        "    \n",
        "    for batch, (data, labels) in enumerate(train_dataloader):\n",
        "        # Transfer Data to GPU if available\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "\n",
        "        # Clear the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Make prediction & compute the mini-batch training loss\n",
        "        preds = resNet50_model(data)\n",
        "        loss = BCEloss(preds, labels.unsqueeze(1))\n",
        "\n",
        "        # Compute the gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Update Weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Aggregate mini-batch training losses\n",
        "        train_loss += loss.item()\n",
        "        train_loss_list.append(train_loss)\n",
        "\n",
        "        loss, current = loss.item(), (batch+1)*len(data)\n",
        "        if batch+1 == len(train_dataloader):\n",
        "            current = size_trainset\n",
        "        if batch == 0 or batch % 10 == 0 or batch+1 == len(train_dataloader):\n",
        "            print(f\"mini-batch loss for training : {loss:>7f}  [{current:>5d}/{size_trainset:>5d}]\")\n",
        "                \n",
        "    # Compute the global training loss as the mean of the mini-batch training losses\n",
        "    train_loss /= len(train_dataloader)\n",
        "    \n",
        "    resNet50_model.eval()\n",
        "    valid_loss = 0.0\n",
        "    correct_results_sum = 0\n",
        "    print('\\n')\n",
        "    # Test part : no gradient update\n",
        "    with torch.no_grad():\n",
        "        for batch, (data, labels) in enumerate(val_dataloader):\n",
        "            # Transfer Data to GPU if available\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            # Forward Pass & compute the mini-batch validation loss\n",
        "            preds = resNet50_model(data)\n",
        "            loss = BCEloss(preds,labels.unsqueeze(1))\n",
        "\n",
        "            # Calculate Loss\n",
        "            valid_loss += loss.item()\n",
        "            valid_loss_list.append(valid_loss)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            preds_binary = torch.round(preds).squeeze(1)\n",
        "            correct_results = (preds_binary == labels).sum().float()\n",
        "            correct_results_sum += correct_results\n",
        "            batch_acc = torch.round(correct_results / len(data))*100\n",
        "\n",
        "            loss, current = loss.item(), (batch+1)*len(data)\n",
        "            if batch+1 == len(train_dataloader):\n",
        "                current = size_valset\n",
        "            if batch == 0 or batch % 2 == 0 or batch+1 == len(val_dataloader):\n",
        "                print(f\"mini-batch for validation : Loss {loss:>7f}    Acc {batch_acc}%    [{current:>5d}/{size_valset:>5d}]\")\n",
        "    \n",
        "    # Compute the global validation loss as the mean of the mini-batch validation losses\n",
        "    valid_loss /= len(val_dataloader)\n",
        "    acc = torch.round(correct_results_sum / size_valset)*100\n",
        "    print('\\n')\n",
        "    print(f\"Epoch {epoch+1} \\n Training Loss: {train_loss:>7f} \\n Validation Loss: {valid_loss:>7f} \\n Accuracy: {acc}%\" )\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "u5IQX8l2RC-4"
      },
      "outputs": [],
      "source": [
        "losses = {'train_loss' : train_loss, 'valid_loss' : valid_loss}\n",
        "with open(\"/Users/thierryksstentini/Downloads/dataset/losses_resNet50_FaceDetection_VSC.pickle\", \"wb\") as file:\n",
        "    pickle.dump(losses, file)\n",
        "\n",
        "torch.save(resNet50_model.state_dict(), '/Users/thierryksstentini/Downloads/dataset/weights_resNet50_FaceDetection_VSC.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing some things"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = resNet50_custom()\n",
        "model.load_state_dict(torch.load('/Users/thierryksstentini/Downloads/dataset/weights_resNet50_FaceDetection_VSC.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {},
      "outputs": [],
      "source": [
        "class resNet50_kustom(torch.nn.Module):\n",
        "    def __init__(self, resnet):\n",
        "        super(resNet50_kustom, self).__init__()\n",
        "        self.new_resnet1 = torch.nn.Sequential(*list(resnet.res50.children())[:6])\n",
        "        self.new_resnet2 = torch.nn.Sequential(*list(resnet.res50.children())[6:-1])\n",
        "        self.new_resnet_fc = torch.nn.Sequential(*list(resnet.res50.children()))[-1]\n",
        "\n",
        "    def forward(self, input):\n",
        "        output1 = self.new_resnet1(input)\n",
        "        output2 = self.new_resnet2(output1)\n",
        "        fc = self.new_resnet_fc(torch.flatten(output2))\n",
        "        return output1, torch.sigmoid(fc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {},
      "outputs": [],
      "source": [
        "resNet50_modelK = resNet50_kustom(resNet50_model.to('cpu'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 512, 2])"
            ]
          },
          "execution_count": 204,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.stack((torch.mean(output1, dim=(2,3)), torch.var(output1, dim=(2,3))), dim=2).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 512, 28, 28]) torch.Size([1])\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    output1, output2 = resNet50_modelK(dataset_train[4][0].unsqueeze(0))\n",
        "    output1moy = torch.mean(output1)\n",
        "    print(output1.shape, output1moy.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "moyenne et variance d'un filtre >>> caractériser l'activation de ce filtre (genre snapshot)\n",
        "image avec une tête >>> activation similaires avec celles de l'entrainement\n",
        "image noire >>> activation complètement différentes avec celles de l'entrainement\n",
        "fitter une gaussienne sur les filtres puis trouver la proba d'avoir tel ou tel filtre\n",
        ">>> si proba trop faible on ignore l'image i.e. c'est un outlayer\n",
        "En fait on cherche les outlayers\n",
        "Peut être que si on est trop tot dans le modele il va y avoir trop de bruit et ça sera nul (on ne pourra pas voir si c'est un outlayer ou non)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Face_detection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "bfc2e043f4a41bbce066f7921047bdcfe8769546c7198e4df6482fd90de274f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
